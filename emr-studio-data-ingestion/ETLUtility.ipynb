{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb8e2c4-d64d-4310-a00a-2905f90d336c",
   "metadata": {},
   "source": [
    "# ETL Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df7103-dac5-4fe6-8641-09aeafabfa24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_date, col, udf\n",
    "\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc216282-a3f0-4746-b761-c1d18abc415f",
   "metadata": {},
   "source": [
    "## Find latest event time from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d1cdc-750e-4be4-b4c5-fa8b5ccce701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_event_time (inputDF, column_name,spark):\n",
    "    inputDF.createOrReplaceTempView(\"df_table\")\n",
    "    sql_string = \"SELECT MAX(\" + column_name + \") as maxval FROM df_table\"\n",
    "    return spark.sql(sql_string).collect()[0].asDict()['maxval']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb5dd6-7d07-4859-b4e4-2adb9f0d7ed4",
   "metadata": {},
   "source": [
    "## Uppercase functions as UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e2ea6-d8bd-47e1-b9ef-b58f7cf00db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf\n",
    "@udf\n",
    "def my_uppercase(my_string):\n",
    "    return my_string.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cc665-4b63-4080-baed-c43451e8d3f2",
   "metadata": {},
   "source": [
    "## Get bookmark value from Redshift Bookmark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9d331-bd14-4a09-990d-793d876c2716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_bookmark_value(v_bookmark_job_id):\n",
    "    \n",
    "    sql = \"select job_id,bookmark_col, bookmark_val from etlfw_rs.bookmark_table where job_id ='\" + v_bookmark_job_id + \"'\"\n",
    "    v_cluster_identifier = 'lakehouse-redshift-cluster'\n",
    "    v_secret_arn = 'arn:aws:secretsmanager:ap-southeast-1:130835040051:secret:prod/redshift-DqEEI9'\n",
    "    v_database_name = 'dev'\n",
    "    \n",
    "    rsd = boto3.client('redshift-data', region_name='ap-southeast-1')\n",
    "\n",
    "    resp = rsd.execute_statement(\n",
    "        Database=v_database_name,\n",
    "        ClusterIdentifier=v_cluster_identifier,\n",
    "        SecretArn=v_secret_arn,\n",
    "        Sql=sql\n",
    "    )\n",
    "    qid = resp[\"Id\"]\n",
    "    \n",
    "    while True:\n",
    "        desc = rsd.describe_statement(Id=qid)\n",
    "        if desc[\"Status\"] == \"FINISHED\":\n",
    "            break\n",
    "            print(desc[\"ResultRows\"])\n",
    "\n",
    "    if desc and desc[\"ResultRows\"]  > 0:\n",
    "        result = rsd.get_statement_result(Id=qid)\n",
    "    \n",
    "    return result[\"Records\"][0][2][\"stringValue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f929-036c-40fe-aac3-544c2675d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update bookmark value with new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eab95c-07e6-4b7b-ab24-8e04e15165ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bookmark_value(v_bookmark_job_id, v_new_bookmark_val):\n",
    "    \n",
    "    sql = \"update etlfw_rs.bookmark_table set bookmark_val='\" + v_new_bookmark_val + \"' where job_id ='\" + v_bookmark_job_id + \"'\"\n",
    "    v_cluster_identifier = 'lakehouse-redshift-cluster'\n",
    "    v_secret_arn = 'arn:aws:secretsmanager:ap-southeast-1:130835040051:secret:prod/redshift-DqEEI9'\n",
    "    v_database_name = 'dev'\n",
    "\n",
    "    rsd = boto3.client('redshift-data', region_name='ap-southeast-1')\n",
    "\n",
    "    resp = rsd.execute_statement(\n",
    "        Database=v_database_name,\n",
    "        ClusterIdentifier=v_cluster_identifier,\n",
    "        SecretArn=v_secret_arn,\n",
    "        Sql=sql\n",
    "    )\n",
    "    qid = resp[\"Id\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff553a-edc1-4590-b7f6-9b189cc041c1",
   "metadata": {},
   "source": [
    "Run the following to convert \n",
    "\n",
    "`jupyter nbconvert ETLUtility.ipynb --to python`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
